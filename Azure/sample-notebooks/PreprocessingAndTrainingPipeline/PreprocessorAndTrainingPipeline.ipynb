{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scikit-Learn Preprocessing and Training Pipeline\n",
        "##### from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "##### from sklearn.naive_bayes import MultinomialNB\n",
        "### Using data from Azure datastore and SAP Datasphere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Install fedml_azure package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1633632022540
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pip install fedml_azure --force-reinstall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the libraries needed in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1633632025593
        }
      },
      "outputs": [],
      "source": [
        "from fedml_azure import DwcAzureTrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up\n",
        "### Creating a Training object and setting the workspace, compute target, and environment.\n",
        "\n",
        "Before running the below cell, ensure that you have a workspace and replace the subscription_id, resource_group, and workspace_name with your information.\n",
        "\n",
        "The whl file for the fedml_azure library must be passed to the pip_wheel_files key in the environment_args and to use scikit-learn, you must pass the name to conda_packages as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1633632030611
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#creation of training object and creating workspace in constructor.\n",
        "\n",
        "training = DwcAzureTrain(\n",
        "                          workspace_args={\"subscription_id\": '<subscription_id>',\n",
        "                                        \"resource_group\": '<resource_group>',\n",
        "                                        \"workspace_name\": '<workspace_name>'\n",
        "                                        },\n",
        "                          experiment_args={'name':'test-2'},\n",
        "                          environment_type='CondaPackageEnvironment',\n",
        "                          environment_args={'name':'test-env-prep','conda_packages':['scikit-learn'],'pip_packages':['fedml_azure']},\n",
        "                          compute_type='AmlComputeCluster',\n",
        "                          compute_args={'vm_size':'Standard_D12_v2',\n",
        "                                'vm_priority':'lowpriority',\n",
        "                                'compute_name':'cpu-clu-prep',\n",
        "                                'min_nodes':0,\n",
        "                                'max_nodes':1,\n",
        "                                'idle_seconds_before_scaledown':1700\n",
        "                                })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Since this model is using data stored in Azure and SAP Datasphere, we need to get the data that was uploaded to Azure so we can pass it to the training script.\n",
        "For information on how this specific data was uploaded to Azure, please refer to `upload_data_to_datastore.ipynb.`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1633632033566
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Dataset, Datastore\n",
        "datastore = Datastore.get(training.workspace, 'workspaceblobstore')\n",
        "datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1633632043603
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, 'dataset/imdb_train.csv')])\n",
        "df = train_dataset.to_pandas_dataframe()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Then, we need to generate the run config. This is needed to package the configuration specified so we can submit a job for training. \n",
        "\n",
        "Before running the following cell, you should have a config.json file with the specified values to allow you to access to SAP Datasphere. Provide this file path to config_file_path in the below cell.\n",
        "\n",
        "You should also have the follow view IMDB_TEST_VIEW created in your SAP Datasphere. To gather this data, please refer to https://www.kaggle.com/mantri7/imdb-movie-reviews-dataset?select=train_data+%281%29.csv and download the test dataset.\n",
        "\n",
        "https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1633632043693
        }
      },
      "outputs": [],
      "source": [
        "#generating the run config\n",
        "src=training.generate_run_config(config_file_path='dwc_configs/config.json',\n",
        "                          config_args={\n",
        "                                          'source_directory':'Scikit-Learn-Preprocessor-Training-Pipeline',\n",
        "                                          'script':'train_script.py',\n",
        "                                          'arguments':[\n",
        "                                              '--model_file_name', 'pipeline.pkl',\n",
        "                                              '--table_name', 'IMDB_TEST_VIEW',\n",
        "                                              '--table_size', 1,\n",
        "                                              '--data', train_dataset.as_named_input('train_data'),\n",
        "                                          ]\n",
        "                                          }\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Submitting the job for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1633632321722
        }
      },
      "outputs": [],
      "source": [
        "#submitting the training run\n",
        "run=training.submit_run(src)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register the model for deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1633391846634
        }
      },
      "outputs": [],
      "source": [
        "model=training.register_model(run=run,\n",
        "                           model_args={'model_name':'sklearn_pipeline_model',\n",
        "                                       'model_path':'outputs/pipeline.pkl'},\n",
        "                            resource_config_args={'cpu':1, 'memory_in_gb':0.5},\n",
        "                            is_sklearn_model=True\n",
        "                           )\n",
        "print('Name:', model.name)\n",
        "print('Version:', model.version)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
