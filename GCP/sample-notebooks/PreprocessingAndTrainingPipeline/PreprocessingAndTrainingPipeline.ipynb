{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Preprocessing and Training Pipeline\n",
    "##### from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "##### from sklearn.naive_bayes import MultinomialNB\n",
    "### Using data from Google Cloud Storage and DWC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install fedml_gcp package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fedml_gcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedml_gcp import DwcGCP\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DwcGCP Instance to access class methods and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is expected that the bucket name passed here already exists in Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwc = DwcGCP(project_name='example-project',\n",
    "                 bucket_name='<bucket-name>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tar bundle of script folder so GCP can use it for training\n",
    "\n",
    "Before running this cell, please ensure that the script package has all the necessary files for a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwc.make_tar_bundle('PreprocessedPipeline.tar.gz', \n",
    "                    'PreprocessingAndTrainingPipeline', \n",
    "                    'preprocessed-pipeline/train/PreprocessedPipeline.tar.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCP takes in training inputs that are specific to the training job and the environment needed.\n",
    "\n",
    "In the training inputs, we are the python module. This is the module that your script package is named, and it references the task.py file inside the script package.\n",
    "\n",
    "We are also passing args which hold the table name to get data from. Before running the following cell, you should have a config.json file in the script package with the specified values to allow you to access to DWC.\n",
    "\n",
    "You should also have the follow view `IMDB_TEST_VIEW` created in your DWC. To gather this data, please refer to https://www.kaggle.com/mantri7/imdb-movie-reviews-dataset?select=train_data+%281%29.csv and download the test dataset.\n",
    "\n",
    "This script also downloads data from Cloud Storage and uses it for training. Please download the train dataset from the link below and upload it to your Cloud Storage bucket (file path = <bucket-name>/data) before proceeding. https://www.kaggle.com/mantri7/imdb-movie-reviews-dataset?select=train_data+%281%29.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = {\n",
    "    'scaleTier': 'BASIC',\n",
    "    'packageUris': ['gs://<bucket-name>/preprocessed-pipeline/train/PreprocessedPipeline.tar.gz'],\n",
    "    'pythonModule': 'trainer.task',\n",
    "    'args': ['--table_name', 'IMDB_TEST_VIEW',\n",
    "             '--table_size', '1',\n",
    "            '--file_path', 'data/imdb_train.csv',\n",
    "            '--bucket_name', '<bucket-name>'],\n",
    "    'region': 'us-east1',\n",
    "    'jobDir': 'gs://<bucket-name>',\n",
    "    'runtimeVersion': '2.5',\n",
    "    'pythonVersion': '3.7',\n",
    "    'scheduling': {'maxWaitTime': '3600s', 'maxRunningTime': '7200s'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwc.train_model('preprocessed_pipeline_final', training_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwc.deploy(model_name='<example-model>', \n",
    "           model_location='/preprocessed-pipeline/model', version='v1', region='us-east1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
